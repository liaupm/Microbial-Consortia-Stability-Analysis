# -*- coding: utf-8 -*-
"""CleanDatasetReplating.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e4ArUoLIIauWWZR6LfXu4qJR_vDLOjOH
"""

import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, minmax_scale, scale

import matplotlib.pyplot as plt
import seaborn as sns
import bokeh as bk

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

INPUT_FILE_NAME = "/gdrive/My Drive/Colab Notebooks/TFM/data_replating.csv"
ATT_FILE_NAME = "/gdrive/My Drive/Colab Notebooks/TFM/data_replating_clean.csv"
ONE_HOT_ENCODED_CLASSES_FILE_NAME = "/gdrive/My Drive/Colab Notebooks/TFM/data_replating_onehot_2clases.csv"
CONTINUOUS_2LABEL_FILE_NAME = "/gdrive/My Drive/Colab Notebooks/TFM/data_replating_continuous_2labels.csv"
CONTINUOUS_FILE_NAME = "/gdrive/My Drive/Colab Notebooks/TFM/data_replating_continuous.csv"

dataset = pd.read_csv(INPUT_FILE_NAME)
dataset.columns

dataset = dataset.drop(['Unnamed: 0', 'Unnamed: 0.1', 'posS1', 'posS2'], axis=1)
dataset.info()

dataset.head(10)

dataset.tail(10)

"""**First Step:** find out whether or not there are missing values. """

{att : dataset[dataset[att].isnull()].shape[0] for att in dataset.columns}

dataset = dataset.fillna(3)
dataset['ratioS1'] = dataset['ratioS1'].replace([1000], 3)
dataset['ratioS2'] = dataset['ratioS2'].replace([1000], 3)

dataset = dataset.drop_duplicates()

"""**Second Step**: Check outliers and scale data. There are several actions to consider regarding outliers:


1.   **Scale** each attribute based on its mean and standard deviation (normalization). This approach may produce values larger than 1 or lower than -1.
2.   **Remove** rows containing outliers; at least some of them. The disadvantage is that we do not have so many examples in the dataset.
3.   **Curate** data by modifying outliers.
"""

normalized_ds = pd.DataFrame (scale (dataset.drop (columns="New_Stability"), axis=0, copy=True),columns=dataset.drop (columns="New_Stability").columns) 
normalized_ds

normalized_ds.boxplot(figsize=(20,7))

standardized_ds = pd.DataFrame (minmax_scale (dataset.drop (columns="New_Stability"), axis=0, copy=True),columns=dataset.drop (columns="New_Stability").columns) 
standardized_ds

standardized_ds.boxplot(figsize=(20,7))

"""Both boxplot figures show the existence of really few outliers. We proceed to curate the data.
**We study each attribute** and apply modifications to outliers to get them into the normal distribution. No lo hago porque no me hace falta.

Let's see the **correlation matrix** and **descriptive statistics** on the dataset. The **correlation matrix** permits to visualize dependencies between pairs of attributes: values close to -1 or +1 indicate a high correlation. A negative correlation value means than when the value of an attribute gets high, the value of the other attribute decreases, and vice-versa. Positive correlation values point out that both features increase or decrease simultaneously.
"""

dataset.corr()

dataset.describe()

sns.displot(dataset["ratioS1"], kde=False, rug=True)

dataset.boxplot(column=['ratioS1'])

print("minimum: ",np.amin(dataset['ratioS1'].values), "Maximum: ", np.amax(dataset['ratioS1'].values))

#dataset.loc[dataset['ratioS1'] < 0.25,'ratioS1']= 0.25
#dataset.boxplot(column=['ratioS1'])

sns.displot(dataset["ratioS2"], kde=False, rug=True)

dataset.boxplot(column=['ratioS2'])

print("minimum: ",np.amin(dataset['ratioS2'].values), "Maximum: ", np.amax(dataset['ratioS2'].values))

"""Check how many instances per label there are and group them if necessary."""

dataset["New_Stability"].value_counts()

dataset.shape

dataset=dataset.sample(frac=1) #frac is the fraction of axis items to return. 1 means all of them
dataset=dataset.sample(frac=1)
dataset=dataset.sample(frac=1).reset_index(drop=True) #Reset index and drop the old one
dataset.head()

t = dataset['New_Stability']
t[:10]

#Guardamos t en el archivo continuous
t.to_csv(CONTINUOUS_FILE_NAME, index = False)

# 6 LABELS #
#dataset.loc[dataset['Stability'] == 0.1 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.2 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.4 ,'Stability']=0.3
#dataset.loc[dataset['Stability'] == 0.6 ,'Stability']=0.7
#dataset.loc[dataset['Stability'] == 0.8 ,'Stability']=0.9
#dataset['Stability'].value_counts()

# 4 LABELS #
#dataset.loc[dataset['Stability'] == 0.1 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.2 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.3 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.4 ,'Stability']=0.5
#dataset.loc[dataset['Stability'] == 0.6 ,'Stability']=0.5
#dataset.loc[dataset['Stability'] == 0.8 ,'Stability']=0.7
#dataset.loc[dataset['Stability'] == 0.9 ,'Stability']=1
#dataset['Stability'].value_counts()

# 2 CLASSES #
dataset.loc[dataset['New_Stability'] < 1.0 ,'New_Stability'] =0
dataset['New_Stability'].value_counts()

#Stability 4 clases
#dataset.loc[dataset['Stability'] == 0.1 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.2 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.3 ,'Stability']=0
#dataset.loc[dataset['Stability'] == 0.4 ,'Stability']=0.5
#dataset.loc[dataset['Stability'] == 0.6 ,'Stability']=0.5
#dataset.loc[dataset['Stability'] == 0.8 ,'Stability']=0.7
#dataset.loc[dataset['Stability'] == 0.9 ,'Stability']=1

dataset.shape

t = dataset.New_Stability
x = dataset.drop(columns='New_Stability')

x.head()

#Guardamos el continuous file 0 y 1
t.to_csv(CONTINUOUS_2LABEL_FILE_NAME, index = False)

"""One-hot encode the classes for a classification problem."""

names = np.array(['Unstable', 'Stable'])
names

encoder = OneHotEncoder(categories="auto", sparse=False) # Function that one-hot encodes integers
one_hot_t = encoder.fit_transform (t.values.reshape(-1,1))
one_hot_t[:10]

one_hot_t = pd.DataFrame(data=one_hot_t,columns=names)
one_hot_t[:10]

"""Min-max scale of the input dataset (attributes) within the range [-1,1] for each feature independently."""

print("ratioS1: Minimum: ",np.amin(dataset['ratioS1'].values), "Maximum: ", np.amax(dataset['ratioS1'].values))
print("ratioS2: Minimum: ",np.amin(dataset['ratioS2'].values), "Maximum: ", np.amax(dataset['ratioS2'].values))
print("Num_Cells_Strain1: Minimum: ",np.amin(dataset['Num_Cells_Strain1'].values), "Maximum: ", np.amax(dataset['Num_Cells_Strain1'].values))
print("Num_Cells_Strain2: Minimum: ",np.amin(dataset['Num_Cells_Strain2'].values), "Maximum: ", np.amax(dataset['Num_Cells_Strain2'].values))
print("t_deg: Minimum: ",np.amin(dataset['t_deg'].values), "Maximum: ", np.amax(dataset['t_deg'].values))
print("k_degA: Minimum: ",np.amin(dataset['k_degA'].values), "Maximum: ", np.amax(dataset['k_degA'].values))
print("k_degB: Minimum: ",np.amin(dataset['k_degB'].values), "Maximum: ", np.amax(dataset['k_degB'].values))

x_s = pd.DataFrame (minmax_scale (x, feature_range=(-1, 1),axis=0, copy=True),columns=x.columns) 
x_s[:10]

x_s.to_csv(ATT_FILE_NAME, index=False)
one_hot_t.to_csv(ONE_HOT_ENCODED_CLASSES_FILE_NAME, index=False)